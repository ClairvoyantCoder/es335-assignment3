{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0dkPyzWLfSx",
        "outputId": "34a29255-8137-4fea-8787-c5f4ac72594e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit==1.39.0 in /usr/local/lib/python3.12/dist-packages (1.39.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (2.32.4)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (4.15.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (6.5.1)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.39.0) (5.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair<6,>=4.0->streamlit==1.39.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6,>=4.0->streamlit==1.39.0) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6,>=4.0->streamlit==1.39.0) (2.10.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.39.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit==1.39.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit==1.39.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit==1.39.0) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit==1.39.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit==1.39.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit==1.39.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit==1.39.0) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14,>=10.14.0->streamlit==1.39.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14,>=10.14.0->streamlit==1.39.0) (2.19.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.39.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair<6,>=4.0->streamlit==1.39.0) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.39.0) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.39.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.39.0) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.39.0) (0.28.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.39.0) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit==1.39.0) (1.17.0)\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "NgrokTunnel: \"https://unpaining-chalkstony-nga.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit==1.39.0 pyngrok\n",
        "!ngrok authtoken 350YoZESCvKz6s2CHsU9cDWvuqf_6LS5cvjH6sZ3DcAYujEFv\n",
        "!streamlit run app.py &> /content/log.txt &\n",
        "from pyngrok import ngrok; print(ngrok.connect(8501))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6eJueuELO_n",
        "outputId": "fc33e2b8-f25a-4bbd-f41b-101b48e8c414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import json\n",
        "import difflib\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import streamlit as st\n",
        "\n",
        "#           SETTINGS\n",
        "APP_TITLE = \"MLP Next-Word/Line Generator\"\n",
        "APP_VERSION = \"1.4.1\"\n",
        "DEFAULT_CKPT_DIR = \"/content/drive/MyDrive/Model_Checkpoints/weights\"  \n",
        "DEFAULT_JSON_DIR  = \"/content/drive/MyDrive/es335-assignment-3\"         \n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#          UTILITIES\n",
        "def set_seed(seed: int):\n",
        "    import random\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def strip_compile_prefix(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    if any(k.startswith(\"_orig_mod.\") for k in sd.keys()):\n",
        "        return {k.replace(\"_orig_mod.\", \"\", 1): v for k, v in sd.items()}\n",
        "    return sd\n",
        "\n",
        "def id2tok_from_inv(inv_vocab: Dict) -> List[str]:\n",
        "    # inv_vocab keys can be str or int\n",
        "    keys = [int(k) for k in inv_vocab.keys()] if inv_vocab and isinstance(next(iter(inv_vocab.keys())), str) else list(inv_vocab.keys())\n",
        "    max_id = max(keys) if keys else 0\n",
        "    arr = [\"\"] * (max_id + 1)\n",
        "    for k, v in inv_vocab.items():\n",
        "        idx = int(k) if isinstance(k, str) else k\n",
        "        if idx < 0:\n",
        "            continue\n",
        "        if idx >= len(arr):\n",
        "            arr.extend([\"\"] * (idx - len(arr) + 1))\n",
        "        arr[idx] = v\n",
        "    for i, t in enumerate(arr):\n",
        "        if t == \"\":\n",
        "            arr[i] = \"<UNK>\"\n",
        "    return arr\n",
        "\n",
        "def best_fuzzy_match(tok: str, vocab: Dict[str, int], cutoff: float = 0.8) -> Optional[str]:\n",
        "    matches = difflib.get_close_matches(tok, vocab.keys(), n=1, cutoff=cutoff)\n",
        "    return matches[0] if matches else None\n",
        "\n",
        "def normalize_token(tok: str) -> str:\n",
        "    return tok.strip()\n",
        "\n",
        "def map_oov_token(tok: str, vocab: Dict[str, int], id2tok: List[str], strategy: str, dataset_hint: str) -> str:\n",
        "    if tok in vocab:\n",
        "        return tok\n",
        "    if strategy == \"fuzzy\":\n",
        "        cand = best_fuzzy_match(tok, vocab, cutoff=0.75)\n",
        "        if cand:\n",
        "            return cand\n",
        "    if strategy == \"skip\":\n",
        "        return \"\"\n",
        "    # fallback\n",
        "    if dataset_hint == \"code\" and \"<NL>\" in vocab:\n",
        "        return \"<NL>\"\n",
        "    if \".\" in vocab:\n",
        "        return \".\"\n",
        "    return id2tok[0] if id2tok else next(iter(vocab.keys()))\n",
        "\n",
        "def encode_context(words: List[str], vocab: Dict[str, int], id2tok: List[str], context_len: int,\n",
        "                   oov_strategy: str, dataset_hint: str) -> torch.Tensor:\n",
        "    pad = \".\" if \".\" in vocab else \"<NL>\" if \"<NL>\" in vocab else (id2tok[0] if id2tok else next(iter(vocab.keys())))\n",
        "    toks = []\n",
        "    for w in words:\n",
        "        w = normalize_token(w)\n",
        "        mapped = map_oov_token(w, vocab, id2tok, oov_strategy, dataset_hint)\n",
        "        if mapped != \"\":\n",
        "            toks.append(mapped)\n",
        "    toks = toks[-context_len:]\n",
        "    toks = ([pad] * (context_len - len(toks))) + toks\n",
        "    idx = [vocab[t] for t in toks]\n",
        "    return torch.tensor([idx], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "#            MODEL\n",
        "class MLPTextGen(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 context_len: int,\n",
        "                 emb_dim: int = 64,\n",
        "                 hidden_layers: int = 1,\n",
        "                 hidden_dim: int = 1024,\n",
        "                 activation: str = \"relu\",\n",
        "                 dropout: float = 0.2,\n",
        "                 use_adaptive_softmax: bool = False,\n",
        "                 adaptive_cutoffs: Optional[List[int]] = None):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.context_len = context_len\n",
        "        self.use_adaptive = use_adaptive_softmax\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        act = nn.ReLU if activation.lower() == \"relu\" else nn.Tanh\n",
        "        layers = [nn.Linear(context_len * emb_dim, hidden_dim), act(), nn.Dropout(dropout)]\n",
        "        if hidden_layers == 2:\n",
        "            layers += [nn.Linear(hidden_dim, hidden_dim), act(), nn.Dropout(dropout)]\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "        self.proj = nn.Linear(hidden_dim, emb_dim, bias=False)\n",
        "\n",
        "        if self.use_adaptive:\n",
        "            if adaptive_cutoffs is None:\n",
        "                c1 = min(20000, vocab_size // 10)\n",
        "                c2 = min(60000, vocab_size // 2)\n",
        "                adaptive_cutoffs = [c for c in [c1, c2] if c < vocab_size]\n",
        "            self.adaptive = nn.AdaptiveLogSoftmaxWithLoss(\n",
        "                in_features=emb_dim, n_classes=vocab_size, cutoffs=adaptive_cutoffs\n",
        "            )\n",
        "            self.decoder = None\n",
        "        else:\n",
        "            self.decoder = nn.Linear(emb_dim, vocab_size, bias=False)\n",
        "            self.decoder.weight = self.emb.weight  # weight tying\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_prob(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        e = self.emb(x)\n",
        "        h = self.mlp(e.reshape(e.size(0), -1))\n",
        "        z = self.proj(h)\n",
        "        if self.use_adaptive:\n",
        "            return self.adaptive.log_prob(z)\n",
        "        else:\n",
        "            return F.log_softmax(self.decoder(z), dim=-1)\n",
        "\n",
        "#       CHECKPOINT LOADING\n",
        "def safe_load_checkpoint(path: Path):\n",
        "    if not path.exists() or not path.is_file():\n",
        "        raise ValueError(f\"Checkpoint not found: {path}\")\n",
        "    try:\n",
        "        ckpt = torch.load(path, map_location=DEVICE)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to load checkpoint: {e}\")\n",
        "\n",
        "    needed = [\"model_state\", \"config\", \"vocab\", \"inv_vocab\"]\n",
        "    if not isinstance(ckpt, dict) or any(k not in ckpt for k in needed):\n",
        "        raise ValueError(\"Checkpoint must contain keys: model_state, config, vocab, inv_vocab\")\n",
        "\n",
        "    sd = strip_compile_prefix(ckpt[\"model_state\"])\n",
        "    cfg = ckpt[\"config\"]; vocab = ckpt[\"vocab\"]; inv_vocab = ckpt[\"inv_vocab\"]\n",
        "\n",
        "    for k in [\"vocab_size\",\"context_len\",\"emb_dim\",\"hidden_layers\",\"hidden_dim\",\"activation\",\"dropout\"]:\n",
        "        if k not in cfg:\n",
        "            raise ValueError(f\"Config missing key: {k}\")\n",
        "\n",
        "    model = MLPTextGen(\n",
        "        vocab_size=cfg[\"vocab_size\"],\n",
        "        context_len=cfg[\"context_len\"],\n",
        "        emb_dim=cfg[\"emb_dim\"],\n",
        "        hidden_layers=cfg[\"hidden_layers\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        activation=cfg[\"activation\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        use_adaptive_softmax=cfg.get(\"use_adaptive_softmax\", False),\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
        "    if missing or unexpected:\n",
        "        # Non-fatal: warn in console\n",
        "        print(\"State dict warnings:\", \"missing:\", missing, \"unexpected:\", unexpected)\n",
        "\n",
        "    model.eval()\n",
        "    return model, cfg, vocab, inv_vocab\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def cached_load(path_str: str):\n",
        "    return safe_load_checkpoint(Path(path_str))\n",
        "\n",
        "def list_checkpoints(folder: str) -> List[Path]:\n",
        "    if not folder or not Path(folder).exists():\n",
        "        return []\n",
        "    return sorted([p for p in Path(folder).glob(\"*.pt\")])\n",
        "\n",
        "#         GENERATION\n",
        "@torch.no_grad()\n",
        "def generate(model: MLPTextGen,\n",
        "             start_words: List[str],\n",
        "             vocab: Dict[str, int],\n",
        "             inv_vocab: Dict[str, str],\n",
        "             k_steps: int = 20,\n",
        "             temperature: float = 1.0,\n",
        "             topk: Optional[int] = 50,\n",
        "             context_len_override: Optional[int] = None,\n",
        "             oov_strategy: str = \"fallback\",\n",
        "             dataset_hint: str = \"natural\",\n",
        "             seed: int = 1337) -> str:\n",
        "    set_seed(seed)\n",
        "    id2tok = id2tok_from_inv(inv_vocab)\n",
        "    context_len = min(int(context_len_override or model.context_len), model.context_len)\n",
        "    out = list(start_words)\n",
        "\n",
        "    for _ in range(k_steps):\n",
        "        x = encode_context(out, vocab, id2tok, context_len, oov_strategy, dataset_hint)\n",
        "        logp = model.log_prob(x) / max(temperature, 1e-8)\n",
        "        if topk is not None and topk > 0 and topk < logp.size(-1):\n",
        "            v, ix = torch.topk(logp, topk)\n",
        "            mask = torch.full_like(logp, float(\"-inf\"))\n",
        "            logp = mask.scatter(1, ix, v)\n",
        "        probs = torch.softmax(logp, dim=-1).squeeze(0)\n",
        "        nxt_id = torch.multinomial(probs, num_samples=1).item()\n",
        "        nxt_tok = inv_vocab[str(nxt_id)] if isinstance(inv_vocab, dict) else id2tok[nxt_id]\n",
        "        out.append(nxt_tok)\n",
        "    return \" \".join(out)\n",
        "\n",
        "#             UI\n",
        "st.set_page_config(page_title=APP_TITLE, page_icon=\"ðŸ§ \", layout=\"wide\")\n",
        "st.title(f\"{APP_TITLE} Â· {APP_VERSION}\")\n",
        "st.caption(\"Load a trained variant, set temperature/top-k/seed/context, and generate next k tokens. OOV-safe.\")\n",
        "\n",
        "# Sidebar:\n",
        "st.sidebar.header(\"Model Variants\")\n",
        "ckpt_dir = st.sidebar.text_input(\"Checkpoint folder\", value=DEFAULT_CKPT_DIR)\n",
        "refresh = st.sidebar.button(\"ðŸ”„ Refresh list\")\n",
        "\n",
        "# Discover checkpoints\n",
        "ckpts = list_checkpoints(ckpt_dir)\n",
        "if not ckpts:\n",
        "    st.warning(\"No checkpoints found in the provided folder. Make sure Drive is mounted and the path is correct.\")\n",
        "    st.info(f\"Expected path example: {DEFAULT_CKPT_DIR}\")\n",
        "    st.stop()\n",
        "\n",
        "# Let user choose a variant (string path so it persists across reruns)\n",
        "variant_path_str = st.sidebar.selectbox(\n",
        "    \"Choose variant (.pt)\", options=[p.as_posix() for p in ckpts], index=0\n",
        ")\n",
        "if not variant_path_str:\n",
        "    st.warning(\"Please select a model variant to continue.\")\n",
        "    st.stop()\n",
        "\n",
        "# Load model safely (cached)\n",
        "try:\n",
        "    with st.spinner(\"Loading modelâ€¦\"):\n",
        "        model, cfg, vocab, inv_vocab = cached_load(variant_path_str)\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to load checkpoint:\\n\\n{e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Only now is it safe to reference cfg/vocab\n",
        "dataset_hint = \"code\" if \"<NL>\" in vocab else \"natural\"\n",
        "st.success(\n",
        "    f\"Loaded  |  Variant: **{Path(variant_path_str).name}**  |  Vocab: **{len(vocab)}**  |  \"\n",
        "    f\"Context len: **{cfg['context_len']}**  |  Adaptive: **{cfg.get('use_adaptive_softmax', False)}**\"\n",
        ")\n",
        "\n",
        "# Sidebar: generation controls (after cfg exists)\n",
        "st.sidebar.header(\"Generation Controls\")\n",
        "context_len_override = st.sidebar.number_input(\n",
        "    \"Context length (â‰¤ model)\", min_value=1, max_value=cfg[\"context_len\"], value=min(5, cfg[\"context_len\"])\n",
        ")\n",
        "temperature = st.sidebar.slider(\"Temperature\", 0.2, 2.0, 1.0, 0.05)\n",
        "topk = st.sidebar.slider(\"Top-k (0=disabled)\", 0, 200, 50, 5)\n",
        "topk = None if topk == 0 else topk\n",
        "k_steps = st.sidebar.number_input(\"Predict next k tokens\", 1, 200, 30)\n",
        "seed = st.sidebar.number_input(\"Random seed\", 1, 999999, 1337)\n",
        "oov_strategy = st.sidebar.selectbox(\"OOV handling\", [\"fallback\", \"fuzzy\", \"skip\"], index=0)\n",
        "\n",
        "# Sidebar: model info \n",
        "st.sidebar.header(\"Model Config\")\n",
        "st.sidebar.write(f\"Embedding dim: **{cfg['emb_dim']}**\")\n",
        "st.sidebar.write(f\"Hidden layers: **{cfg['hidden_layers']}**\")\n",
        "st.sidebar.write(f\"Hidden size: **{cfg['hidden_dim']}**\")\n",
        "st.sidebar.write(f\"Activation: **{cfg['activation']}**\")\n",
        "st.sidebar.write(f\"Dropout: **{cfg['dropout']}**\")\n",
        "\n",
        "# Main: input & generation \n",
        "st.subheader(\"Input\")\n",
        "default_prompt = \"to sherlock holmes she is\" if dataset_hint == \"natural\" else \"for ( i = 0;\"\n",
        "user_text = st.text_input(\"Enter context (space-separated tokens):\", value=default_prompt)\n",
        "\n",
        "c1, c2, c3 = st.columns([1,1,1])\n",
        "with c1:\n",
        "    do_generate = st.button(\"ðŸš€ Generate\")\n",
        "with c2:\n",
        "    show_vocab = st.checkbox(\"Show vocab size\", value=False)\n",
        "with c3:\n",
        "    show_oov = st.checkbox(\"Show OOV mapping example\", value=False)\n",
        "\n",
        "if do_generate:\n",
        "    tokens = [t for t in user_text.strip().split() if t]\n",
        "    t0 = time.time()\n",
        "    try:\n",
        "        out = generate(\n",
        "            model=model,\n",
        "            start_words=tokens,\n",
        "            vocab=vocab,\n",
        "            inv_vocab=inv_vocab,\n",
        "            k_steps=int(k_steps),\n",
        "            temperature=float(temperature),\n",
        "            topk=topk,\n",
        "            context_len_override=int(context_len_override),\n",
        "            oov_strategy=oov_strategy,\n",
        "            dataset_hint=dataset_hint,\n",
        "            seed=int(seed),\n",
        "        )\n",
        "        dt = time.time() - t0\n",
        "        st.subheader(\"Output\")\n",
        "        st.code(out)\n",
        "        st.caption(f\"Generated in {dt:.2f}s on {DEVICE.type}\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Generation failed: {e}\")\n",
        "\n",
        "if show_vocab:\n",
        "    st.info(f\"Vocabulary size: **{len(vocab)}**  Â·  Dataset: **{dataset_hint}**\")\n",
        "\n",
        "if show_oov:\n",
        "    sample_oov = \"Sherloc\" if dataset_hint == \"natural\" else \"memcoy\"\n",
        "    mapped = map_oov_token(sample_oov, vocab, id2tok_from_inv(inv_vocab), oov_strategy, dataset_hint)\n",
        "    st.write(f\"OOV example: `{sample_oov}` â†’ `{mapped}` (strategy: **{oov_strategy}**)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeN50F_bLVBu",
        "outputId": "fa3db4cf-033a-48f3-d508-c1e5bc7d0933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "NgrokTunnel: \"https://unpaining-chalkstony-nga.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 350YoZESCvKz6s2CHsU9cDWvuqf_6LS5cvjH6sZ3DcAYujEFv\n",
        "!streamlit run app.py &> /content/log.txt &\n",
        "from pyngrok import ngrok; print(ngrok.connect(8501))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8angTUCzYTGk",
        "outputId": "5e41cf88-faa0-4aaa-ecba-651dc75cdf0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE3hEgdxZutN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
